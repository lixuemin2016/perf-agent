#!/usr/bin/env python3

# Description:
#   This script is used to pick up the testcases.
#
# Maintainers:
#   Charles Shih <schrht@gmail.com>
#

"""pick testcases.

This script is used to pick up the testcases.
It analyse the benchmark report generated by perf-insight, filter out the 
testcases, restore their parameters, and dump them into a backlog file.
"""

import argparse
import logging
import toml
import json
import os

LOG = logging.getLogger(__name__)
logging.basicConfig(level=logging.DEBUG, format='%(levelname)s: %(message)s')

ARG_PARSER = argparse.ArgumentParser(
    description="Pick up the testcases from benchmark report.")

ARG_PARSER.add_argument(
    '--report-id',
    dest='report_id',
    action='store',
    help='The Benchmark Report ID generated by perf-insight.',
    default=None,
    required=True)
ARG_PARSER.add_argument(
    '--case-filter',
    dest='case_filters',
    action='append',
    help='The conclusion(s) for filtering testcases.',
    default=[],
    required=False)
ARG_PARSER.add_argument(
    '--backlog-file',
    dest='backlog_file',
    action='store',
    help='The backlog file to store the testcases.',
    default='backlog.toml',
    required=False)


if __name__ == '__main__':

    # Parse parameters
    ARGS = ARG_PARSER.parse_args()

    report_id = ARGS.report_id
    backlog_file = ARGS.backlog_file
    case_filters = ARGS.case_filters if ARGS.case_filters else [
        'Dramatic Regression']

    # Get the statistics from the benchmark report
    LOG.info('Getting the statistics from the benchmark report...')

    cmd = 'picli --output-format json benchmark-inspect \
        --get-statistics true --report-id {}'.format(report_id)

    try:
        with os.popen(cmd) as p:
            _output = p.readlines()
            benchmark = json.loads(''.join(_output))
    except Exception as err:
        LOG.error(
            'Failed to inspect the specified benchmark report: {}'.format(err))
        exit(1)

    statistics = benchmark.get('statistics')
    if statistics is None:
        LOG.error('This benchmark report does not enable statistics function.')
        exit(1)

    testcases = statistics.get('benchmark', {})

    LOG.info('Got {} testcase(s).'.format(len(testcases)))

    # Filter testcases
    LOG.info('Filtering testcases...')
    LOG.debug('Conclusion Filter: {}'.format(case_filters))

    testcases = [x for x in testcases if x.get(
        'Conclusion') in case_filters]

    LOG.info('Got {} testcase(s).'.format(len(testcases)))

    # Restore test parameters
    LOG.info('Restoring test parameters...')

    testcases_args = []
    for case in testcases:
        if 'RW' in case:
            # pbench-fio parameters
            args = {
                'CASE_ID':  case.get('CaseID'),
                'test-types': case.get('RW'),
                'block-sizes': case.get('BS'),
                'iodepth': case.get('IOdepth'),
                'numjobs': case.get('Numjobs')
            }
        elif '' in case:
            # pbench-uperf parameters
            # TODO: implement it
            args = {}
        else:
            LOG.error('Unrecognized TestRun Type.')
            exit(1)

        LOG.debug(args)
        testcases_args.append(args)

    LOG.info('Restored {} testcase(s).'.format(len(testcases_args)))

    # Write to the backlog file
    LOG.info('Writing to the backlog file...')

    try:
        with open(ARGS.backlog_file, 'w') as f:
            toml.dump({'testcases': testcases_args}, f)
    except Exception as err:
        LOG.error(
            'Failed to write to the backlog file: {}'.format(err))
        exit(1)

    LOG.info('Wrote {} testcase(s) to file "{}".'.format(
        len(testcases_args), ARGS.backlog_file))

    exit(0)
